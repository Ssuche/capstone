{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,make_scorer,precision_recall_curve,average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "data = pd.read_csv(\"../processed_data/merged_data.csv\")\n",
    "\n",
    "list_bp = ['avg_dbp', 'avg_diff', 'avg_sbp', 'max_sbp']\n",
    "list_ed = ['age', 'sex', 'language', 'insurance_type', 'primary_care', \n",
    "            'ed_name', 'bpa_response', 'htn_on_pl', 'htn_on_pmh', \n",
    "            'hld_on_pl', 'hld_on_pmh', 'family_dm', 'tobacco_user', \n",
    "            'htn_meds', 'statin_meds', 'disposition', 'detailed_race', \n",
    "            'weight', 'bmi', 'hba1c', 'height', 'sbp_1st', 'dbp_1st', \n",
    "            'poct_gluc']\n",
    "list_lab = ['max_value_GLUCOSE', 'avg_value_GLUCOSE', 'max_value_CREATININE', \n",
    "            'min_value_CREATININE', 'min_value_GLUCOSE',  'avg_value_CREATININE', \n",
    "            'avg_value_HEMOGLOBIN A1C', 'max_value_HEMOGLOBIN A1C', 'min_value_HEMOGLOBIN A1C',  \n",
    "            'min_value_GLUCOSE, POC', 'avg_value_GLUCOSE, POC', 'max_value_GLUCOSE, POC']\n",
    "list_geo = [\n",
    "    'po_box', 'homeless', 'total_pop', 'households', 'housing_units', \n",
    "    'p_children', 'p_elderly', 'p_adults', 'p_female', 'mdn_age', \n",
    "    'p_nhwhite', 'p_nhblack', 'p_hispanic', 'p_nhasian', 'p_other', \n",
    "    'p_moved', 'p_longcommute', 'p_marriednone', 'p_marriedkids', \n",
    "    'p_singlenone', 'p_malekids', 'p_femalekids', 'p_cohabitkids', \n",
    "    'p_nohsdeg', 'p_hsonly', 'p_somecollege', 'p_collegeplus', \n",
    "    'p_onlyenglish', 'p_spanishlimited', 'p_asianlimited', 'p_otherlimited', \n",
    "    'p_limitedall', 'p_notlimited', 'p_popbelow1fpl', 'p_popbelow2fpl', \n",
    "    'p_povmarriedfam', 'p_povmalefam', 'p_povfemalefam', 'hh_mdnincome', \n",
    "    'p_pubassist', 'p_foodstamps', 'p_assistorfood', 'p_unemployed', \n",
    "    'h_vacant', 'h_renter', 'h_occupants', 'h_novehicles', 'h_mdnrent', \n",
    "    'h_rentpercent', 'h_houseprice', 'p_private', 'p_medicare', 'p_medicaid', \n",
    "    'p_otherinsur', 'p_uninsured', 'h_nointernet', 'h_nocomputer', \n",
    "    'p_foreign', 'p_disabled']\n",
    "list_visit = ['visit_type']\n",
    "\n",
    "\n",
    "lists = list_bp+ list_ed+ list_lab+ list_geo+ list_visit\n",
    "X_all = data[lists]\n",
    "y = data['pcp_followup'].map({'Yes': 1, 'No': 0})\n",
    "y = np.array(y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "\n",
    "numeric_cols = X_all.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X_all.select_dtypes(exclude=['number']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X_all)\n",
    "if hasattr(X_preprocessed, \"toarray\"):\n",
    "    X_preprocessed = X_preprocessed.toarray()\n",
    "\n",
    "\n",
    "numeric_feature_names = numeric_cols \n",
    "cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "all_feature_names = list(numeric_feature_names) + list(cat_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected by importance\n",
    "# select tail 10 features\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=50)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=50)\n",
    "\n",
    "lasso = Lasso(alpha=1e-3,\n",
    "              max_iter=1035,\n",
    "              tol=1e-10,\n",
    "              random_state=50,\n",
    "              selection='cyclic',)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "coef = lasso.coef_\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': coef\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "selected_feature_names = feature_importance[feature_importance['importance'] < 0]\n",
    "selected_feature_names = selected_feature_names.tail(10)\n",
    "X_train = pd.DataFrame(X_train, columns=all_feature_names)[selected_feature_names['feature']]\n",
    "X_val = pd.DataFrame(X_val, columns=all_feature_names)[selected_feature_names['feature']]\n",
    "X_test= pd.DataFrame(X_test, columns=all_feature_names)[selected_feature_names['feature']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(accuracy_score, average='macro')\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=50), \n",
    "    param_grid, \n",
    "    cv=5,  \n",
    "    scoring=scorer, \n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.47\n"
     ]
    }
   ],
   "source": [
    "y_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_val, \n",
    "    y_proba,\n",
    "    pos_label=1)\n",
    "\n",
    "average_accuracies  = []\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    acc_0 = np.mean(y_pred[y_val == 0] == 0) \n",
    "    acc_1 = np.mean(y_pred[y_val == 1] == 1)  \n",
    "    avg_acc = (acc_0 + acc_1) / 2\n",
    "    average_accuracies.append(avg_acc)\n",
    "\n",
    "best_index = np.argmax(average_accuracies)\n",
    "optimal_threshold = thresholds[best_index]\n",
    "print(f\"Optimal threshold: {optimal_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Confusion Matrix:\n",
      "[[ 3  9]\n",
      " [ 3 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.25      0.33        12\n",
      "           1       0.74      0.90      0.81        29\n",
      "\n",
      "    accuracy                           0.71        41\n",
      "   macro avg       0.62      0.57      0.57        41\n",
      "weighted avg       0.67      0.71      0.67        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = np.where(y_proba > optimal_threshold, 1, 0)\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "Confusion Matrix:\n",
      "[[ 7  5]\n",
      " [18 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.58      0.38        12\n",
      "           1       0.81      0.54      0.65        39\n",
      "\n",
      "    accuracy                           0.55        51\n",
      "   macro avg       0.54      0.56      0.51        51\n",
      "weighted avg       0.68      0.55      0.58        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, \n",
    "    y_proba,\n",
    "    pos_label=1)\n",
    "average_accuracies  = []\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    acc_0 = np.mean(y_pred[y_test == 0] == 0) \n",
    "    acc_1 = np.mean(y_pred[y_test == 1] == 1)  \n",
    "    avg_acc = (acc_0 + acc_1) / 2\n",
    "    average_accuracies.append(avg_acc)\n",
    "\n",
    "best_index = np.argmax(average_accuracies)\n",
    "optimal_threshold = thresholds[best_index]\n",
    "\n",
    "y_test_pred = np.where(y_proba > optimal_threshold, 1, 0)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
